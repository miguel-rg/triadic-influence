{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguel-rg/triadic-influence/blob/main/triadic_influence_data_analysis_and_training_NN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqjyVk1gk42K"
      },
      "source": [
        "# Notebook to reproduce the results presented in \n",
        "\n",
        "Triadic influence as a proxy for compatibility in social relationships, Ruíz-García, M., Ozaita, J., Pereda, M., Alfonso, A., Branas-Garza, P., Cuesta, J. A., and Sánchez, A., Proceedings of the National Academy of Sciences, 120 (13) e2215041120\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0udc2eHn8K4M"
      },
      "source": [
        "Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OouMRMsdFcx"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "import networkx as nx\n",
        "import pylab\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw6WpwAi8Oq6"
      },
      "source": [
        "Downloading the data from Zenodo (https://zenodo.org/record/7647000#.Y-8kEezMLPC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_AG5JnKFm4n"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUVg33zb8Qpt"
      },
      "outputs": [],
      "source": [
        "specific_schools = ['t11_10', 't11_9', 't11_8', 't11_7', 't11_6', 't11_5', 't11_4', 't11_3', 't11_2', 't11_1', 't1', 't2', 't6']\n",
        "\n",
        "for school_id in specific_schools:\n",
        "    wget.download('https://zenodo.org/record/7647000/files/Nodes_{}.csv?download=1'.format(school_id))\n",
        "\n",
        "for school_id in specific_schools:\n",
        "    wget.download('https://zenodo.org/record/7647000/files/Edges_{}.csv?download=1'.format(school_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a3VruaJcj9e"
      },
      "outputs": [],
      "source": [
        "dict_nodes = {}\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".csv\") and file[0:5]=='Nodes':\n",
        "        dict_nodes[file[6:-4]] = pd.read_csv(file)\n",
        "\n",
        "dict_edges = {}\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".csv\") and file[0:5]=='Edges':\n",
        "        dict_edges[file[6:-4]] = pd.read_csv(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "424BsEi7rbgO"
      },
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD6ReSUowpOw"
      },
      "source": [
        "Remove students that have Nan values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZegUDpzVAuox"
      },
      "outputs": [],
      "source": [
        "nodes_clean_dict = {}\n",
        "total_removed_students = 0\n",
        "total_students = 0\n",
        "for key_ind in specific_schools: \n",
        "    \n",
        "    nodes_i = dict_nodes[key_ind]\n",
        "    nodes_s = nodes_i[[\"ID\",\"prosocial\",\"Sexo\",\"crttotal\"]] \n",
        "    # REMOVE STUDENTS WITH NANs\n",
        "    nodes_clean = nodes_s.dropna(subset=[\"ID\",\"prosocial\",\"Sexo\",\"crttotal\"])\n",
        "    ### DROPPED STUDENTS\n",
        "    only_na_mh_col_3 = nodes_s[~nodes_s.index.isin(nodes_clean.index)]\n",
        "    removed_students_mh_col_3 = list(only_na_mh_col_3.ID)\n",
        "    total_removed_students += len(removed_students_mh_col_3)\n",
        "    total_students += np.shape(nodes_s)[0]\n",
        "    node_feature_dim= nodes_clean.shape[1]-1\n",
        "    nodes_clean_dict[key_ind] = nodes_clean\n",
        "\n",
        "print('total_removed_students',total_removed_students)\n",
        "print('total_students',total_students)\n",
        "print('total_students-total_removed_students',total_students-total_removed_students)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00EljOnlxl7r"
      },
      "source": [
        "Check that the IDs are unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx4uld-7dUfb"
      },
      "outputs": [],
      "source": [
        " ### STUDENTS WITH THE SAME ID??? ###\n",
        "\n",
        "nodes_id_test = np.array([])\n",
        "for key_ind in dict_nodes.keys():\n",
        "    nodes_i = dict_nodes[key_ind]\n",
        "    nodes_id_test = np.concatenate((nodes_id_test,np.array(nodes_i[[\"ID\"]].values.flatten())) )#\" Future \",\" CRT \",\n",
        "\n",
        "uuu, ccc = np.unique(nodes_id_test, return_counts=True)\n",
        "dup = uuu[ccc > 1]\n",
        "\n",
        "print('REPEATED STUDENTS?', dup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvIZD8N9y_F3"
      },
      "source": [
        "Keep only the edges that connect nodes with personal information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f54NOZvf3e0T"
      },
      "outputs": [],
      "source": [
        "\n",
        "dir_edges_all_dict = {}\n",
        "\n",
        "for key_ind in specific_schools: #dict_nodes.keys():\n",
        "\n",
        "    edges_i = dict_edges[key_ind]\n",
        "    nodes_i = nodes_clean_dict[key_ind]\n",
        "\n",
        "    # SOME STUDENTS APPEAR IN THE EDGES BUT NOT IN THE NODE FILES\n",
        "    all_nodes_in_edges = np.unique(list(edges_i['from'])+list(edges_i['to']))\n",
        "\n",
        "    students_school = list(nodes_i['ID'])\n",
        "    removed_students_extra = list(set(students_school).symmetric_difference(set(all_nodes_in_edges)))\n",
        "\n",
        "    #keep only edges connecting nodes with personal information\n",
        "    data_edges_clean = edges_i[(~edges_i['from'].isin(removed_students_extra) & ~edges_i['to'].isin(removed_students_extra))]\n",
        "\n",
        "    e_from_col_1 = list(data_edges_clean[\"from\"])\n",
        "    e_to_col_1 = list(data_edges_clean[\"to\"])\n",
        "    e_weight_col_1 = list(data_edges_clean[\"weight\"])\n",
        "\n",
        "    dir_edges_all = [(e_from_col_1[aaa],e_to_col_1[aaa],e_weight_col_1[aaa]) for aaa in range(len(e_from_col_1))]\n",
        "    dir_edges_all_dict[key_ind] = dir_edges_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjue9GtlzUyx"
      },
      "source": [
        "Create dictionaries for each school"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNmd-MyHc4ks"
      },
      "outputs": [],
      "source": [
        "\n",
        "Student_Id_dict = {}\n",
        "prosocial_dict = {}\n",
        "honestidad_dict = {}\n",
        "crttotal_dict = {}\n",
        "Sexo_dict = {}\n",
        "\n",
        "for key_ind in specific_schools:\n",
        "    print('key_ind',key_ind)\n",
        "    Student_Id_dict[key_ind]  = list(nodes_clean_dict[key_ind].ID)\n",
        "    prosocial_dict[key_ind] = [round(float(aaa),2) for aaa in list(nodes_clean_dict[key_ind][\"prosocial\"])]\n",
        "    crttotal_dict[key_ind] = [float(aaa) for aaa in list(nodes_clean_dict[key_ind][\"crttotal\"])]\n",
        "    Sexo_dict[key_ind] = [1.0 if aaa == 'Male' else 0.0 for aaa in list(nodes_clean_dict[key_ind][\"Sexo\"])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm3-REPozmWw"
      },
      "source": [
        "Create lists containing the data from all the schools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRrbAD6sdRTa"
      },
      "outputs": [],
      "source": [
        "Student_Id_all = []\n",
        "prosocial_all = []\n",
        "honestidad_all = []\n",
        "crttotal_all = []\n",
        "Sexo_all = []\n",
        "\n",
        "for key_ind in Student_Id_dict.keys():\n",
        "\n",
        "    Student_Id_all += Student_Id_dict[key_ind]\n",
        "    prosocial_all += prosocial_dict[key_ind]\n",
        "    crttotal_all += crttotal_dict[key_ind]\n",
        "    Sexo_all += Sexo_dict[key_ind]\n",
        "\n",
        "\n",
        "all_edges = []\n",
        "for key_ind in dir_edges_all_dict.keys():\n",
        "    all_edges += dir_edges_all_dict[key_ind]\n",
        "\n",
        "\n",
        "#sorting the lists by prosocial skills\n",
        "sorted_lists_all = [[x, y, w1, w2] for x, y, w1, w2 in sorted(zip(Student_Id_all, prosocial_all, crttotal_all, Sexo_all), key=lambda pair: pair[1])] \n",
        "\n",
        "Student_Id_all_sort = [aaa[0] for aaa in sorted_lists_all]\n",
        "prosocial_all_sort = [aaa[1] for aaa in sorted_lists_all]\n",
        "crttotal_all_sort = [aaa[2] for aaa in sorted_lists_all]\n",
        "Sexo_all_sort = [aaa[3] for aaa in sorted_lists_all]\n",
        "\n",
        "#create mapping to find the index of each student\n",
        "mapping_Ids = {index: i for i, index in enumerate(Student_Id_all_sort)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1xGLG0Y0U3K"
      },
      "source": [
        "Define some parameters for the style of the plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5GA9GSw4dpu"
      },
      "outputs": [],
      "source": [
        "sns.set(context='talk', font_scale=1.75,  color_codes=True, palette='deep', style='ticks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUhUSOKGrotb"
      },
      "source": [
        "### Analyzing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUU4Spo203L8"
      },
      "source": [
        "Percentage of each kind of edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlWrCFtK07TO"
      },
      "outputs": [],
      "source": [
        "labels_create_percentages = np.array([ aaa for aaa in np.array(all_edges)[:,2]])\n",
        "print(' Enemy edges: ', 100*np.shape(np.where(labels_create_percentages.flatten() == -2))[1]/np.shape(labels_create_percentages.flatten())[0],'%')\n",
        "print(' Dislike edges: ', 100*np.shape(np.where(labels_create_percentages.flatten() == -1))[1]/np.shape(labels_create_percentages.flatten())[0],'%')\n",
        "print(' Like edges: ', 100*np.shape(np.where(labels_create_percentages.flatten() == 1))[1]/np.shape(labels_create_percentages.flatten())[0],'%')\n",
        "print(' Friends edges: ', 100*np.shape(np.where(labels_create_percentages.flatten() == 2))[1]/np.shape(labels_create_percentages.flatten())[0],'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4zSwDXR1Jy7"
      },
      "source": [
        "Plotting the proportion of the different relationships in each school"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akjgbvgsdtEz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "len_keys = len(dir_edges_all_dict.keys())\n",
        "color = iter(plt.cm.plasma(np.linspace(0, 1, len_keys)))\n",
        "\n",
        "bins_hist = np.array([-2,-1,0,1,2])\n",
        "\n",
        "fig_relations = plt.figure(figsize = (20,14))\n",
        "ind_pl = 0\n",
        "num_bars = len(dir_edges_all_dict.keys())\n",
        "width_pl = 1/(2*num_bars)\n",
        "print('width_pl', width_pl)\n",
        "\n",
        "for key_ind in dir_edges_all_dict.keys():\n",
        "    labels_create_percentages = np.array([ aaa for aaa in np.array(dir_edges_all_dict[key_ind] )[:,2]])\n",
        "    plot_hist = [100*np.shape(np.where(labels_create_percentages.flatten() == -2))[1]/np.shape(labels_create_percentages.flatten())[0], 100*np.shape(np.where(labels_create_percentages.flatten() == -1))[1]/np.shape(labels_create_percentages.flatten())[0],100*np.shape(np.where(labels_create_percentages.flatten() == 0))[1]/np.shape(labels_create_percentages.flatten())[0], 100*np.shape(np.where(labels_create_percentages.flatten() == 1))[1]/np.shape(labels_create_percentages.flatten())[0], 100*np.shape(np.where(labels_create_percentages.flatten() == 2))[1]/np.shape(labels_create_percentages.flatten())[0]]\n",
        "    color_pl = next(color)\n",
        "    plt.bar(bins_hist-num_bars/2*width_pl+width_pl*ind_pl, plot_hist, width = width_pl, label = key_ind, color = color_pl)\n",
        "    ind_pl += 1\n",
        "\n",
        "plt.ylabel('%')\n",
        "plt.xlabel('relations')\n",
        "plt.xticks([-2,-1,1,2], [-2,-1,1,2]) \n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO6--wv3WmxB"
      },
      "source": [
        "Studying the prosociality of the students"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO3rUNp3WsrV"
      },
      "outputs": [],
      "source": [
        "\n",
        "len_keys = len(dir_edges_all_dict.keys())\n",
        "color = iter(plt.cm.plasma(np.linspace(0, 1, len_keys)))\n",
        "\n",
        "fig_prosc = plt.figure(figsize = (20,14))\n",
        "ind_pl = 0\n",
        "num_bars = len(dir_edges_all_dict.keys())\n",
        "width_pl = 1/(2*num_bars)/4\n",
        "print('width_pl', width_pl)\n",
        "bins_hist = np.array([0., 0.33, 0.67, 1.])\n",
        "\n",
        "for key_ind in dir_edges_all_dict.keys():\n",
        "    labels_create_percentages = np.array(prosocial_dict[key_ind])\n",
        "      \n",
        "    plot_hist = [100*np.shape(np.where(labels_create_percentages.flatten() == 0))[1]/np.shape(labels_create_percentages.flatten())[0], 100*np.shape(np.where(labels_create_percentages.flatten() == 0.33))[1]/np.shape(labels_create_percentages.flatten())[0],100*np.shape(np.where(labels_create_percentages.flatten() == 0.67))[1]/np.shape(labels_create_percentages.flatten())[0], 100*np.shape(np.where(labels_create_percentages.flatten() == 1))[1]/np.shape(labels_create_percentages.flatten())[0]]\n",
        "    color_pl = next(color)\n",
        "    plt.bar(bins_hist-num_bars/2*width_pl+width_pl*ind_pl, plot_hist, width = width_pl, label = key_ind, color = color_pl)\n",
        "    ind_pl += 1\n",
        "\n",
        "\n",
        "plt.xticks([0,0.33,0.66,1], [0,0.33,0.66,1]) #, rotation='vertical')\n",
        "\n",
        "plt.ylabel('%')\n",
        "plt.xlabel('Prosociality')\n",
        "# plt.title('Prosociality in each school')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# fig_prosc.savefig(GD_folder_figures+ '/Prosociality_each_school_new.pdf', bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM8KMGV_2SPA"
      },
      "source": [
        "Create matrices with relationships between students"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YmABIyyeANh"
      },
      "outputs": [],
      "source": [
        "\n",
        "### MATRICES FOR ALL THE SCHOOLS MERGED ###\n",
        "\n",
        "#weighted adjacency matrix\n",
        "adj_matrix_relations = np.zeros((len(Student_Id_all_sort),len(Student_Id_all_sort)))\n",
        "\n",
        "#asymmetric adjacency matrix (only ones if there is a directed relation)\n",
        "adj_matrix_ones = np.zeros((len(Student_Id_all_sort),len(Student_Id_all_sort)))\n",
        "\n",
        "all_edges = np.array(all_edges)\n",
        "\n",
        "for iii in range(len(all_edges)):\n",
        "\n",
        "    adj_matrix_relations[mapping_Ids[all_edges[iii,0]], mapping_Ids[all_edges[iii,1]]] = all_edges[iii,2]\n",
        "    \n",
        "    if all_edges[iii,2] != 0:\n",
        "        adj_matrix_ones[mapping_Ids[all_edges[iii,0]], mapping_Ids[all_edges[iii,1]]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUtJq8_A3gJm"
      },
      "source": [
        "Studying the number of relationships between the different gender combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhKrvIvzbhzT"
      },
      "outputs": [],
      "source": [
        "### Plotting proportion of male/female friend for male and female students\n",
        "\n",
        "ind_males = np.where(np.array(Sexo_all_sort) == 1)[0]\n",
        "\n",
        "total_num_male_friends_male_st = 0\n",
        "total_num_female_friends_male_st = 0\n",
        "total_num_male_enemies_male_st = 0\n",
        "total_num_female_enemies_male_st = 0\n",
        "\n",
        "for male_st in ind_males:\n",
        "    friends_of_male_st = np.where(adj_matrix_relations[male_st,:] > 0)[0]\n",
        "    gender_of_friends = [Sexo_all_sort[aaa] for aaa in friends_of_male_st]\n",
        "    num_female_friends = gender_of_friends.count(0)\n",
        "    num_male_friends = gender_of_friends.count(1)\n",
        "    total_num_male_friends_male_st += num_male_friends\n",
        "    total_num_female_friends_male_st += num_female_friends\n",
        "\n",
        "    enemies_of_male_st = np.where(adj_matrix_relations[male_st,:] < 0)[0]\n",
        "    gender_of_enemies = [Sexo_all_sort[aaa] for aaa in enemies_of_male_st]\n",
        "    num_female_enemies = gender_of_enemies.count(0)\n",
        "    num_male_enemies = gender_of_enemies.count(1)\n",
        "    total_num_male_enemies_male_st += num_male_enemies\n",
        "    total_num_female_enemies_male_st += num_female_enemies\n",
        "\n",
        "\n",
        "ind_females = np.where(np.array(Sexo_all_sort) == 0)[0]\n",
        "\n",
        "total_num_male_friends_female_st = 0\n",
        "total_num_female_friends_female_st = 0\n",
        "total_num_male_enemies_female_st = 0\n",
        "total_num_female_enemies_female_st = 0\n",
        "\n",
        "for female_st in ind_females:\n",
        "    friends_of_female_st = np.where(adj_matrix_relations[female_st,:] > 0)[0]\n",
        "    gender_of_friends = [Sexo_all_sort[aaa] for aaa in friends_of_female_st]\n",
        "    num_female_friends = gender_of_friends.count(0)\n",
        "    num_male_friends = gender_of_friends.count(1)\n",
        "    total_num_male_friends_female_st += num_male_friends\n",
        "    total_num_female_friends_female_st += num_female_friends\n",
        "\n",
        "    enemies_of_female_st = np.where(adj_matrix_relations[female_st,:] < 0)[0]\n",
        "    gender_of_enemies = [Sexo_all_sort[aaa] for aaa in enemies_of_female_st]\n",
        "    num_female_enemies = gender_of_enemies.count(0)\n",
        "    num_male_enemies = gender_of_enemies.count(1)\n",
        "    total_num_male_enemies_female_st += num_male_enemies\n",
        "    total_num_female_enemies_female_st += num_female_enemies\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "langs = ['Male friends', 'Female friends', 'Male enemies', 'Female enemies']\n",
        "students = [total_num_male_friends_male_st,total_num_female_friends_male_st,total_num_male_enemies_male_st,total_num_female_enemies_male_st]\n",
        "ax.bar(langs,students)\n",
        "\n",
        "rects = ax.patches\n",
        "for rect, label in zip(rects, students):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width() / 2, height + 5, label, ha=\"center\", va=\"bottom\", fontsize=12)\n",
        "\n",
        "plt.title('Male students declare')\n",
        "plt.ylabel('Num. relations')\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "langs = ['Male friends', 'Female friends', 'Male enemies', 'Female enemies']\n",
        "students = [total_num_male_friends_female_st,total_num_female_friends_female_st,total_num_male_enemies_female_st,total_num_female_enemies_female_st]\n",
        "ax.bar(langs,students)\n",
        "plt.title('Female students declare')\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "rects = ax.patches\n",
        "for rect, label in zip(rects, students):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width() / 2, height + 5, label, ha=\"center\", va=\"bottom\", fontsize=12)\n",
        "\n",
        "plt.ylabel('Num. relations')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "langs = ['Male friends', 'Female friends', 'Male enemies', 'Female enemies']\n",
        "\n",
        "total_relations_with_male = total_num_male_friends_male_st + total_num_male_enemies_male_st\n",
        "total_relations_with_female = total_num_female_friends_male_st + total_num_female_enemies_male_st\n",
        "\n",
        "students = 100*np.array([total_num_male_friends_male_st/total_relations_with_male,total_num_female_friends_male_st/total_relations_with_female,total_num_male_enemies_male_st/total_relations_with_male,total_num_female_enemies_male_st/total_relations_with_female])\n",
        "ax.bar(langs,students)\n",
        "\n",
        "rects = ax.patches\n",
        "for rect, label in zip(rects, students):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width() / 2, height + 0.1, round(label), ha=\"center\", va=\"bottom\", fontsize=12)\n",
        "\n",
        "plt.title('Male students declare')\n",
        "plt.ylabel('Percent. relations')\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "langs = ['Male friends', 'Female friends', 'Male enemies', 'Female enemies']\n",
        "total_relations_with_male = total_num_male_friends_female_st + total_num_male_enemies_female_st\n",
        "total_relations_with_female = total_num_female_friends_female_st + total_num_female_enemies_female_st\n",
        "\n",
        "students = 100*np.array([total_num_male_friends_female_st/total_relations_with_male,total_num_female_friends_female_st/total_relations_with_female,total_num_male_enemies_female_st/total_relations_with_male,total_num_female_enemies_female_st/total_relations_with_female])\n",
        "\n",
        "\n",
        "ax.bar(langs,students)\n",
        "plt.title('Female students declare')\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "rects = ax.patches\n",
        "for rect, label in zip(rects, students):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width() / 2, height + 0.1, round(label), ha=\"center\", va=\"bottom\", fontsize=12)\n",
        "\n",
        "plt.ylabel('Percent. relations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC1iJYEtNyEG"
      },
      "source": [
        "Computing the triadic influence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhKu2EVWAzsH"
      },
      "outputs": [],
      "source": [
        "## COMPUTING A MATRIX WITH INFORMATION ABOUT WHAT YOUR FIRST ORDER CONTACTS TELL YOU OF SOMEONE ELSE\n",
        "friend_influence = np.dot(adj_matrix_relations,adj_matrix_relations)\n",
        "np.fill_diagonal(friend_influence, 0.0) #remove the diagonal\n",
        "\n",
        "## COMPUTING A MATRIX COUNTING THE NUMBER OF DIRECTED PATHS OF LENGTH 2 (IT IS NOT SYMMETRIC)\n",
        "counting_first_contacts = np.dot(adj_matrix_ones,adj_matrix_ones)\n",
        "np.fill_diagonal(counting_first_contacts, 0.0) #remove the diagonal\n",
        "\n",
        "## Create a list with the number of directed paths of length 2 between the nodes forming each edge\n",
        "list_num_contacts = []\n",
        "for iii in range(len(all_edges)):\n",
        "    list_num_contacts += [counting_first_contacts[mapping_Ids[all_edges[iii,0]], mapping_Ids[all_edges[iii,1]]] ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JhHAmAiXrde"
      },
      "source": [
        "Understanding if the prosociality of the student that recives or declares a relationship affects the sign of the relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf2bmzQVf4os"
      },
      "outputs": [],
      "source": [
        "\n",
        "students_with_prosoc_col_3 = np.array(Student_Id_all_sort)\n",
        "social_skills_col_3 = np.array(prosocial_all_sort)\n",
        "all_relations_col_3 = np.zeros((len(students_with_prosoc_col_3),6))\n",
        "\n",
        "for ind in range(len(students_with_prosoc_col_3)):\n",
        "\n",
        "  relations_from_col_3 = np.where(all_edges[:,0]==students_with_prosoc_col_3[ind])[0]\n",
        "  relations_values_col_3 = np.array([all_edges[aaa,2] for aaa in relations_from_col_3])\n",
        "  #Number of people they consider their friends\n",
        "  num_friends_from_col_3 = len(np.where(relations_values_col_3>0)[0])\n",
        "  #Number of people they consider their enemies\n",
        "  num_enemies_from_col_3 = len(np.where(relations_values_col_3<0)[0])\n",
        "\n",
        "  relations_to_col_3 = np.where(all_edges[:,1]==students_with_prosoc_col_3[ind])[0]\n",
        "  relations_values_col_3 = np.array([all_edges[aaa,2] for aaa in relations_to_col_3])\n",
        "  #Number of people that consider them their friends\n",
        "  num_friends_to_col_3 = len(np.where(relations_values_col_3>0)[0])\n",
        "  #Number of people that consider them their enemies\n",
        "  num_enemies_to_col_3 = len(np.where(relations_values_col_3<0)[0])\n",
        "\n",
        "  all_relations_col_3[ind,0] = students_with_prosoc_col_3[ind]\n",
        "  all_relations_col_3[ind,1] = social_skills_col_3[ind]\n",
        "  all_relations_col_3[ind,2] = num_friends_from_col_3\n",
        "  all_relations_col_3[ind,3] = num_enemies_from_col_3\n",
        "  all_relations_col_3[ind,4] = num_friends_to_col_3\n",
        "  all_relations_col_3[ind,5] = num_enemies_to_col_3\n",
        "\n",
        "\n",
        "siz1 = 12\n",
        "siz2 = 6\n",
        "regul_div = 0.1 # regularizer for the division of friends over enemies\n",
        "\n",
        "x_plot_col_3 = all_relations_col_3[:,1]\n",
        "x_plot_u_col_3 = np.unique(x_plot_col_3)\n",
        "y_plot_col_3 = all_relations_col_3[:,2]\n",
        "y_pl_av_col_3 = []\n",
        "y_pl_std_col_3 = []\n",
        "for x_ind_col_3 in x_plot_u_col_3:\n",
        "  inds_x_col_3 = np.where(x_plot_col_3==x_ind_col_3)[0]\n",
        "  y_pl_av_col_3 += [np.mean([y_plot_col_3[aaa] for aaa in inds_x_col_3])]\n",
        "  y_pl_std_col_3 += [np.std([y_plot_col_3[aaa] for aaa in inds_x_col_3])/np.sqrt(len(inds_x_col_3))]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(siz1,siz2))\n",
        "axs.errorbar(x_plot_u_col_3, y_pl_av_col_3, fmt='o', ls='None', yerr=y_pl_std_col_3, capsize=10)\n",
        "axs.set_xlabel('Prosocial')\n",
        "axs.set_ylabel('Number friends (from)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "x_plot_col_3 = all_relations_col_3[:,1]\n",
        "x_plot_u_col_3 = np.unique(x_plot_col_3)\n",
        "y_plot_col_3 = all_relations_col_3[:,3]\n",
        "y_pl_av_col_3 = []\n",
        "y_pl_std_col_3 = []\n",
        "for x_ind_col_3 in x_plot_u_col_3:\n",
        "  inds_x_col_3 = np.where(x_plot_col_3==x_ind_col_3)[0]\n",
        "  y_pl_av_col_3 += [np.mean([y_plot_col_3[aaa] for aaa in inds_x_col_3])]\n",
        "  y_pl_std_col_3 += [np.std([y_plot_col_3[aaa] for aaa in inds_x_col_3])/np.sqrt(len(inds_x_col_3))]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(siz1,siz2))\n",
        "axs.errorbar(x_plot_u_col_3, y_pl_av_col_3, fmt='o', ls='None', yerr=y_pl_std_col_3, capsize=10)\n",
        "axs.set_xlabel('Prosocial')\n",
        "axs.set_ylabel('Number enemies (from)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "x_plot_col_3 = all_relations_col_3[:,1]\n",
        "x_plot_u_col_3 = np.unique(x_plot_col_3)\n",
        "y_plot_col_3 = all_relations_col_3[:,4]\n",
        "y_pl_av_col_3 = []\n",
        "y_pl_std_col_3 = []\n",
        "for x_ind_col_3 in x_plot_u_col_3:\n",
        "  inds_x_col_3 = np.where(x_plot_col_3==x_ind_col_3)[0]\n",
        "  y_pl_av_col_3 += [np.mean([y_plot_col_3[aaa] for aaa in inds_x_col_3])]\n",
        "  y_pl_std_col_3 += [np.std([y_plot_col_3[aaa] for aaa in inds_x_col_3])/np.sqrt(len(inds_x_col_3))]\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(siz1,siz2))\n",
        "axs.errorbar(x_plot_u_col_3, y_pl_av_col_3, fmt='o', ls='None', yerr=y_pl_std_col_3, capsize=10)\n",
        "axs.set_xlabel('Prosocial')\n",
        "axs.set_ylabel('Number friends (to)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "x_plot_col_3 = all_relations_col_3[:,1]\n",
        "x_plot_u_col_3 = np.unique(x_plot_col_3)\n",
        "y_plot_col_3 = all_relations_col_3[:,5]\n",
        "y_pl_av_col_3 = []\n",
        "y_pl_std_col_3 = []\n",
        "for x_ind_col_3 in x_plot_u_col_3:\n",
        "  inds_x_col_3 = np.where(x_plot_col_3==x_ind_col_3)[0]\n",
        "  y_pl_av_col_3 += [np.mean([y_plot_col_3[aaa] for aaa in inds_x_col_3])]\n",
        "  y_pl_std_col_3 += [np.std([y_plot_col_3[aaa] for aaa in inds_x_col_3])/np.sqrt(len(inds_x_col_3))]\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(siz1,siz2))\n",
        "axs.errorbar(x_plot_u_col_3, y_pl_av_col_3, fmt='o', ls='None', yerr=y_pl_std_col_3, capsize=10)\n",
        "axs.set_xlabel('Prosocial')\n",
        "axs.set_ylabel('Number enemies (to)')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7GXRBP2dYLq"
      },
      "source": [
        "Percentage of relationships that connect two nodes that are also connected by a certain number of directed paths of length 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMHQlIRkdgHF"
      },
      "outputs": [],
      "source": [
        "\n",
        "len_keys = len(dir_edges_all_dict.keys())\n",
        "color = iter(plt.cm.plasma(np.linspace(0, 1, len_keys)))\n",
        "\n",
        "fig_num_relations = plt.figure(figsize=(20,14))\n",
        "\n",
        "bins_hist = [aaa for aaa in range(0,40,1)]\n",
        "\n",
        "number_edges = []\n",
        "\n",
        "x_violin = []\n",
        "y_violin = []\n",
        "key_vio = []\n",
        "list_num_contacts_pl_vio = []\n",
        "\n",
        "for key_ind in dir_edges_all_dict.keys():\n",
        "    \n",
        "    edges_pl = np.array(dir_edges_all_dict[key_ind])\n",
        "\n",
        "    list_num_contacts_pl = []\n",
        "    for iii in range(len(edges_pl)):\n",
        "        list_num_contacts_pl += [counting_first_contacts[mapping_Ids[edges_pl[iii,0]], mapping_Ids[edges_pl[iii,1]]] ]\n",
        "\n",
        "    color_pl = next(color)\n",
        "\n",
        "    bin_vio = plt.hist(list_num_contacts_pl, bins=bins_hist,  density=True, histtype=u'step', label =  key_ind, color = color_pl)\n",
        "    \n",
        "    x_violin += [bin_vio[0]]\n",
        "    y_violin += [bin_vio[1]]\n",
        "    list_num_contacts_pl_vio += [list_num_contacts_pl]\n",
        "    \n",
        "    key_vio += [key_ind]\n",
        "\n",
        "    number_edges += [len(list_num_contacts_pl)]\n",
        "\n",
        "plt.hist(list_num_contacts, bins=bins_hist,  density=True, histtype=u'step', linewidth=2.5 , color= 'k')\n",
        "\n",
        "plt.ylabel('% of relations')\n",
        "plt.xlabel('#  paths of length 2 '+ r'$\\left( \\sum_k  A_{ik}A_{kj} \\right)$')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "y_total = []\n",
        "x_total = []\n",
        "for iii in range(len(list_num_contacts_pl_vio)):\n",
        "    y_total += list_num_contacts_pl_vio[iii]\n",
        "    x_total += [key_vio[iii]] * len(list_num_contacts_pl_vio[iii])\n",
        "\n",
        "\n",
        "fig_vio = plt.figure(figsize=(26,12))\n",
        "sns.violinplot(y=y_total, x=x_total ,  palette = [aaa for aaa in plt.cm.plasma(np.linspace(0, 1, len_keys))])#,  palette=['tomato', 'cornflowerblue']) #palette=plt.cm.plasma(np.linspace(0, 1, len_keys))#\n",
        "plt.ylabel('#  paths of length 2 '+ r'$\\left( \\sum_k  A_{ik}A_{kj} \\right)$')\n",
        "plt.xlabel('Schools')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHlnttw3Obl6"
      },
      "source": [
        "# Training the Neural Network (NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-CKI8e3bg_Y"
      },
      "source": [
        "Defining the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IqRZA7DObl6"
      },
      "outputs": [],
      "source": [
        "\n",
        "Softmax_com = torch.nn.Softmax(dim=1)\n",
        "\n",
        "class Net_4(nn.Module):\n",
        "\n",
        "    # NN definition\n",
        "    def __init__(self,train_with, add_friend_influence,seed,nn_input_dim,nn_width,nn_output_dim):\n",
        "        super(Net_4, self).__init__()\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        self.ly1 = nn.Linear(nn_input_dim, nn_width)  \n",
        "        self.ly2 = nn.Linear(nn_width, nn_width)\n",
        "        self.ly3 = nn.Linear(nn_width, nn_output_dim)\n",
        "        self.train_with = train_with\n",
        "        self.add_friend_influence = add_friend_influence\n",
        "\n",
        "    def forward(self, x, linear = False):\n",
        "        x = self.training_attributes_2(x)\n",
        "        x = self.ly1(x)\n",
        "        if not linear:\n",
        "            x = F.relu(x)\n",
        "        x = self.ly3(x)\n",
        "        return x \n",
        "    \n",
        "    # some funtions that we will use to plot the predictions\n",
        "    def input_skills_no_inf(self,X,Y):\n",
        "        x = torch.FloatTensor([[X[aaa],Y[aaa]] for aaa in range(X.shape[0])])\n",
        "        x = self.ly1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.ly3(x)\n",
        "        return Softmax_com(x)\n",
        "        \n",
        "    def input_skills_only_inf(self,X):\n",
        "        x = torch.FloatTensor([[X[aaa]] for aaa in range(X.shape[0])])\n",
        "        x = self.ly1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.ly3(x)\n",
        "        return Softmax_com(x) \n",
        "      \n",
        "    # Merging the information about the students that we will use to predict relationships   \n",
        "    def training_attributes_2(self, x):\n",
        "\n",
        "        attributes = []\n",
        "\n",
        "        if 'prosocial' in self.train_with:\n",
        "            attributes = np.array([[prosocial_sort[x[aaa,0]],prosocial_sort[x[aaa,1]]] for aaa in range(x.shape[0])])\n",
        "\n",
        "        if 'CRT' in self.train_with:\n",
        "            if len(attributes) == 0:\n",
        "                attributes = np.array([[CRT_sort[x[aaa,0]],CRT_sort[x[aaa,1]]] for aaa in range(x.shape[0])])\n",
        "            else:\n",
        "                attributes = np.hstack((attributes,np.array([[CRT_sort[x[aaa,0]],CRT_sort[x[aaa,1]]] for aaa in range(x.shape[0])])))\n",
        "\n",
        "        if 'Sexo' in self.train_with:\n",
        "            if len(attributes) == 0:\n",
        "                attributes = np.array([[Sexo_sort[x[aaa,0]],Sexo_sort[x[aaa,1]]] for aaa in range(x.shape[0])])\n",
        "            else:\n",
        "                attributes = np.hstack((attributes,np.array([[Sexo_sort[x[aaa,0]],Sexo_sort[x[aaa,1]]] for aaa in range(x.shape[0])])))\n",
        "\n",
        "        if self.add_friend_influence:\n",
        "            if len(attributes) == 0:\n",
        "                attributes = np.array([[friend_influence_torch[x[aaa,0],x[aaa,1]]] for aaa in range(x.shape[0])])\n",
        "            else:\n",
        "                attributes = np.hstack((attributes,np.array([[friend_influence_torch[x[aaa,0],x[aaa,1]]] for aaa in range(x.shape[0])])))\n",
        "\n",
        "        return torch.FloatTensor(attributes) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL7JgT0XbkHY"
      },
      "source": [
        "Defining some functions used during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2MjAIdpW6bj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#computing the accuracy of the model\n",
        "def calc_accuracy(mdl,X,Y):\n",
        "    max_vals, max_indices = torch.max(mdl(X),1)\n",
        "    train_acc = (max_indices == Y).sum().data.numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "#computing the weighted (balanced) accuracy of the model\n",
        "def calc_accuracy_weighted(mdl,X,Y):\n",
        "    max_vals, max_indices = torch.max(mdl(X),1)\n",
        "    classes_l = np.unique(Y)\n",
        "\n",
        "    t_or_f = (max_indices == Y)\n",
        "    acc=0\n",
        "    for ii in classes_l:\n",
        "        ind_c = np.where(Y==ii)[0]\n",
        "        acc_1 = np.array([t_or_f[aaa] for aaa in ind_c]).sum()/len(ind_c)\n",
        "        acc += acc_1\n",
        "    acc = acc/len(classes_l)\n",
        "    return acc\n",
        "\n",
        "# Only used if considering dynamical loss functions (http://proceedings.mlr.press/v139/ruiz-garcia21a/ruiz-garcia21a.pdf) \n",
        "# this function creates the oscillations in the dynamical loss function\n",
        "def c_fn(t, i, w_max, T, C):\n",
        "    slope = 2 * (w_max - 1) / T\n",
        "    w_main_class = np.where(t < T / 2., 1 + t * slope, 2 * w_max - t * slope - 1)\n",
        "    res = np.ones(C) + (w_main_class - 1) * np.eye(C)[i]\n",
        "    res = res / np.sum(res) * C\n",
        "    return torch.tensor(res, dtype=torch.float)\n",
        "\n",
        "\n",
        "#plotting the probabilities learnt by the NN\n",
        "titles = ['Probability Enemies','Probability Friends']\n",
        "def plot_probabilities_2(net_1,axs_1,fig,train_with):\n",
        "  \n",
        "  number_points_grid = 25\n",
        "  x = np.linspace(0, 1, number_points_grid)\n",
        "  y = np.linspace(0, 1, number_points_grid)\n",
        "  X, Y = np.meshgrid(x, y)\n",
        "\n",
        "  Z = net_1.input_skills_no_inf(X.flatten(), Y.flatten()).detach().numpy()\n",
        "  \n",
        "  Z = Z.reshape(number_points_grid,number_points_grid,Z.shape[1])\n",
        "\n",
        "  for ind_prob in range(0,Z.shape[2]):\n",
        "      max_val = np.max(Z[:,:,ind_prob])\n",
        "      min_val = np.min(Z[:,:,ind_prob])\n",
        "      im = axs_1[ind_prob].contourf(X, Y, Z[:,:,ind_prob], 30, cmap='plasma' )# **kw) # , vmin=0.1, vmax=0.4) #RdGy\n",
        "      axs_1[ind_prob].set_xlabel(train_with[0] + ' (from)')\n",
        "      axs_1[ind_prob].set_ylabel(train_with[0] + ' (to)')\n",
        "      axs_1[ind_prob].set_title(titles[ind_prob])\n",
        "\n",
        "      divider = make_axes_locatable(axs_1[ind_prob])\n",
        "      cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "      cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "      cbar.set_ticks([])\n",
        "      cax.text(0.15, 1.01, str(round(max_val,2)), transform=cax.transAxes)\n",
        "      cax.text(0.15, -0.02, str(round(min_val,2)), transform=cax.transAxes)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uism0j-0cDIZ"
      },
      "source": [
        "Training the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYEJKHnkW6bj"
      },
      "outputs": [],
      "source": [
        "def train_NN_new(x_two_cl_train_torch, y_two_cl_train_torch, x_two_cl_test_torch, y_two_cl_test_torch, train_with = [ ], add_friend_influence = True, proportion_train_test = 1, n_epochs = 100, w_max_osc =  1, num_periods = 40,learning_rate = 0.1, gamma_LR = 0.99, Linear_TF = False, num_minibatches = 2, batch_size = 20):\n",
        "    \n",
        "    # w_max_osc =  1 corresponds to the standard cross entropy loss (no dynamical loss function)\n",
        "    \n",
        "    D_folder_results = '.'\n",
        "    x_two_cl_train = x_two_cl_train_torch.detach().numpy()\n",
        "    y_two_cl_train = y_two_cl_train_torch.detach().numpy()\n",
        "    x_two_cl_test = x_two_cl_test_torch.detach().numpy()\n",
        "    y_two_cl_test = y_two_cl_test_torch.detach().numpy()\n",
        "\n",
        "    node_feature_dim = len(train_with)\n",
        "\n",
        "    if add_friend_influence:\n",
        "      nn_input_dim = 2*node_feature_dim +1\n",
        "    else:\n",
        "      nn_input_dim = 2*node_feature_dim \n",
        "\n",
        "    nn_width = 100\n",
        "    nn_output_dim = 2\n",
        "    total_time_steps = n_epochs*num_minibatches \n",
        "    period_osc = total_time_steps/num_periods\n",
        "    no_osc_last_epochs = 4\n",
        "    snapshot_every = 10\n",
        "    number_points_grid = 40\n",
        "\n",
        "    seed_list = range(1)\n",
        "\n",
        "    print('n_epochs',n_epochs,'total_time_steps',total_time_steps,'period_osc',period_osc, 'batch_size', batch_size)\n",
        "\n",
        "    parameters = {'nn_width': nn_width,\n",
        "                  'nn_input_dim': nn_input_dim,\n",
        "                  'nn_output_dim': nn_output_dim,\n",
        "                  'learning_rate': learning_rate,\n",
        "                  'n_epochs': n_epochs,\n",
        "                  'batch_size': batch_size,\n",
        "                  'snapshot_every':snapshot_every,\n",
        "                  'number_points_grid':number_points_grid,\n",
        "                  'gamma_LR':gamma_LR,\n",
        "                  'add_friend_influence':add_friend_influence}\n",
        "\n",
        "    now = datetime.now() # current date and time\n",
        "    date_time = now.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
        "\n",
        "    with open(GD_folder_results + '/'+date_time+'_parameters.pkl', 'wb') as handle:\n",
        "        pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    trained_nets = []\n",
        "    loss_list = []\n",
        "    loss_list_test = []\n",
        "    loss_save_uw_list = []\n",
        "    w_max_osc_save = w_max_osc\n",
        "\n",
        "    ind_enemies = np.array(np.where(y_two_cl_train == 0)[0])\n",
        "    ind_friends = np.array(np.where(y_two_cl_train == 1)[0]) \n",
        "\n",
        "    print('starting simulation', 'np.shape(loss_list)',np.shape(loss_list),'np.shape(loss_list_test)',np.shape(loss_list_test))\n",
        "\n",
        "    for seed_ind in seed_list:\n",
        "\n",
        "        print('NEW SIMULATION. SEED: ', seed_ind)\n",
        "\n",
        "        net = Net_4(train_with, add_friend_influence, seed_ind,nn_input_dim,nn_width,nn_output_dim)\n",
        "\n",
        "        print('NN initialized ')\n",
        "\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate) #, momentum=0.9)\n",
        "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_LR)\n",
        "\n",
        "        loss_save = []\n",
        "        loss_test_save = []\n",
        "        loss_save_uw = []\n",
        "        tr_accuracy_save = []\n",
        "        test_accuracy_save = []\n",
        "        tr_accuracy_save_w = []\n",
        "        test_accuracy_save_w = []\n",
        "\n",
        "\n",
        "        fig, axs = plt.subplots(nrows=1, ncols=7, figsize=(42,6))\n",
        "      \n",
        "        titles = ['Probaility Enemies','Probaility Dislike','Probaility Indiferent','Probaility Like','Probaility Friends']\n",
        "\n",
        "        tt = 0\n",
        "        ii = 0\n",
        "        cc = 0\n",
        "        w_max_osc = w_max_osc_save\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            for i in range(num_minibatches):\n",
        "\n",
        "                ## half the batch for each class\n",
        "                x_ene = [x_two_cl_train[iii] for iii in np.random.choice(ind_enemies, size=int(batch_size/2))]\n",
        "                x_fr =  [x_two_cl_train[iii] for iii in np.random.choice(ind_friends, size=int(batch_size/2))]\n",
        "\n",
        "                y_ene = [y_two_cl_train[iii] for iii in np.random.choice(ind_enemies, size=int(batch_size/2))]\n",
        "                y_fr =  [y_two_cl_train[iii] for iii in np.random.choice(ind_friends, size=int(batch_size/2))]\n",
        "\n",
        "                batch_x, batch_y = torch.tensor(x_ene+x_fr, dtype=torch.int32) , torch.tensor(y_ene+y_fr, dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net.forward(batch_x, linear = Linear_TF)\n",
        "\n",
        "                if tt > period_osc:\n",
        "\n",
        "                    tt = 0\n",
        "                    cc = (cc + 1) % nn_output_dim\n",
        "\n",
        "                    print('New Period. Focus on class: ', cc)\n",
        "\n",
        "                    if no_osc_last_epochs:\n",
        "\n",
        "                        if ii > total_time_steps - no_osc_last_epochs*period_osc:\n",
        "                            print('last period without oscillations')\n",
        "                            w_max_osc = 1\n",
        "                \n",
        "                lossfunction = nn.CrossEntropyLoss(weight=c_fn(tt, cc, w_max_osc, period_osc, nn_output_dim)) # if w_max_osc =  1 -> no weighting\n",
        "                loss = lossfunction(outputs,batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                tt += 1\n",
        "                ii += 1\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            tr_accuracy_w = calc_accuracy_weighted(net,x_two_cl_train_torch,y_two_cl_train_torch)\n",
        "            tr_accuracy = calc_accuracy(net,x_two_cl_train_torch,y_two_cl_train_torch)\n",
        "\n",
        "            outputs = net.forward(x_two_cl_train_torch)\n",
        "            loss_forsaving = lossfunction(outputs,y_two_cl_train_torch)\n",
        "\n",
        "            lossfunction_uw = nn.CrossEntropyLoss()\n",
        "            loss_forsaving_uw = lossfunction_uw(outputs,y_two_cl_train_torch)\n",
        "\n",
        "            loss_save += [loss_forsaving.detach().numpy()]\n",
        "            loss_save_uw += [loss_forsaving_uw.detach().numpy()]\n",
        "\n",
        "            tr_accuracy_save += [tr_accuracy]\n",
        "            tr_accuracy_save_w += [tr_accuracy_w]\n",
        "\n",
        "            if proportion_train_test != 1:\n",
        "                test_accuracy_w = calc_accuracy_weighted(net,x_two_cl_test_torch,y_two_cl_test_torch)\n",
        "                test_accuracy = calc_accuracy(net,x_two_cl_test_torch,y_two_cl_test_torch)\n",
        "                outputs_test = net.forward(x_two_cl_test_torch)\n",
        "                loss_forsaving_test = lossfunction(outputs_test,y_two_cl_test_torch)\n",
        "                loss_test_save += [loss_forsaving_test.detach().numpy()]\n",
        "                test_accuracy_save += [test_accuracy]\n",
        "                test_accuracy_save_w += [test_accuracy_w]\n",
        "\n",
        "\n",
        "            if tt % 10==0:\n",
        "                print('epoch', epoch, 'LR', optimizer.param_groups[0]['lr'], 'loss', loss_forsaving_uw, 'training  acc: ', tr_accuracy)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(loss_save)\n",
        "        plt.plot(loss_test_save)\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('loss')\n",
        "        ax = plt.gca()\n",
        "        ax.set_yscale('log')\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(tr_accuracy_save)\n",
        "        plt.plot(test_accuracy_save)\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('Accuracies')\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(tr_accuracy_save_w)\n",
        "        plt.plot(test_accuracy_save_w)\n",
        "        plt.xlabel('epochs')\n",
        "        plt.ylabel('Weighted Accuracies')\n",
        "        plt.show()\n",
        "\n",
        "        trained_nets += [net]\n",
        "\n",
        "        print('before saving', 'np.shape(loss_list)',np.shape(loss_list),'np.shape(loss_test_save)',np.shape(loss_test_save))\n",
        "\n",
        "        loss_list += [loss_save]\n",
        "        loss_list_test += [loss_test_save]\n",
        "        loss_save_uw_list += [loss_save_uw]\n",
        "\n",
        "        print('after saving', 'np.shape(loss_list)',np.shape(loss_list),'np.shape(loss_list_test)',np.shape(loss_list_test))\n",
        "\n",
        "        if len(train_with)==1 and add_friend_influence == False:\n",
        "\n",
        "            fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
        "            plot_probabilities_2(net,axs,fig,train_with)\n",
        "            plt.show()\n",
        "\n",
        "            date_time = now.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
        "            fig.savefig(GD_folder_figures+ '/' + date_time + '_prob_friend_enemies_prosocial.pdf', bbox_inches='tight')\n",
        "\n",
        "    return tr_accuracy_save, tr_accuracy_save_w, test_accuracy_save, test_accuracy_save_w, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvjNEgjfuyph"
      },
      "source": [
        "### Preparing the data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73WrEUNUfr82"
      },
      "source": [
        "Separating relationships that connect two students with a number of directed paths of length 2 between them below/above a certain threshold (we will use threshold=1). Therefore we separate the relationships which nodes are not connected by directed paths of length 2 from the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8A8XExzxEFN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def separating_data_weak_strong(threshold_connection,proportion_train_test_weak,proportion_train_test_strong,data_edges_f,mapping_Ids,counting_first_contacts):\n",
        "    \n",
        "    x_data_strong_connection = []\n",
        "    y_data_strong_connection = []\n",
        "\n",
        "    x_data_weak_connection = []\n",
        "    y_data_weak_connection = []\n",
        "\n",
        "    for iii in range(len(data_edges_f)):\n",
        "\n",
        "        student_from = data_edges_f[iii,0]\n",
        "        student_to = data_edges_f[iii,1]\n",
        "        weight_s = data_edges_f[iii,2]\n",
        "      \n",
        "        if counting_first_contacts[mapping_Ids[student_from], mapping_Ids[student_to]] < threshold_connection:\n",
        "          \n",
        "          x_data_weak_connection += [[mapping_Ids[student_from],mapping_Ids[student_to]]]\n",
        "          y_data_weak_connection += [[weight_s+2]]\n",
        "\n",
        "        else:\n",
        "\n",
        "          x_data_strong_connection += [[mapping_Ids[student_from],mapping_Ids[student_to]]]\n",
        "          y_data_strong_connection += [[weight_s+2]]\n",
        "    \n",
        "    print('np.shape(x_data_weak_connection)',np.shape(x_data_weak_connection))\n",
        "    print('np.shape(y_data_weak_connection)',np.shape(y_data_weak_connection))\n",
        "    print('np.shape(x_data_strong_connection)',np.shape(x_data_strong_connection))\n",
        "    print('np.shape(y_data_strong_connection)',np.shape(y_data_strong_connection))    \n",
        "\n",
        "    ### NUMBER OF EDGES OF EACH CLASS WEAK CONNECTION###\n",
        "    x_data_weak_connection = np.array(x_data_weak_connection)\n",
        "    y_data_weak_connection = np.array(y_data_weak_connection)\n",
        "    x_data_strong_connection = np.array(x_data_strong_connection)\n",
        "    y_data_strong_connection = np.array(y_data_strong_connection)\n",
        "\n",
        "    if threshold_connection >= 1:\n",
        "\n",
        "        print('\\n weak connection')\n",
        "        print(' Enemy edges: ', 100*np.shape(np.where(y_data_weak_connection.flatten() == 0))[1]/np.shape(y_data_weak_connection.flatten())[0],'%')\n",
        "        print(' Dislike edges: ', 100*np.shape(np.where(y_data_weak_connection.flatten() == 1))[1]/np.shape(y_data_weak_connection.flatten())[0],'%')\n",
        "        print(' Indiferent edges: ', 100*np.shape(np.where(y_data_weak_connection.flatten() == 2))[1]/np.shape(y_data_weak_connection.flatten())[0],'%')\n",
        "        print(' Like edges: ', 100*np.shape(np.where(y_data_weak_connection.flatten() == 3))[1]/np.shape(y_data_weak_connection.flatten())[0],'%')\n",
        "        print(' Friends edges: ', 100*np.shape(np.where(y_data_weak_connection.flatten() == 4))[1]/np.shape(y_data_weak_connection.flatten())[0],'%')\n",
        "\n",
        "\n",
        "        num_edges_data = np.shape(y_data_weak_connection)[0]\n",
        "\n",
        "        data_inds = random.sample(range(num_edges_data), num_edges_data)\n",
        "        data_inds_train = data_inds[0:int(proportion_train_test_weak*num_edges_data)]\n",
        "        data_inds_test = data_inds[int(proportion_train_test_weak*num_edges_data):]\n",
        "\n",
        "        x_data_weak_connection_train = x_data_weak_connection[data_inds_train,:]\n",
        "        x_data_weak_connection_test = x_data_weak_connection[data_inds_test,:]\n",
        "\n",
        "        y_data_weak_connection_train = y_data_weak_connection[data_inds_train]\n",
        "        y_data_weak_connection_test = y_data_weak_connection[data_inds_test]\n",
        "\n",
        "        x_data_weak_connection_train = torch.tensor(x_data_weak_connection_train, dtype=torch.int32)\n",
        "        x_data_weak_connection_test = torch.tensor(x_data_weak_connection_test, dtype=torch.int32)\n",
        "\n",
        "        y_data_weak_connection_train = torch.tensor(y_data_weak_connection_train.flatten(), dtype=torch.long)\n",
        "        y_data_weak_connection_test = torch.tensor(y_data_weak_connection_test.flatten(), dtype=torch.long)\n",
        "\n",
        "    print('\\n strong connection')\n",
        "    print(' Enemy edges: ', 100*np.shape(np.where(y_data_strong_connection.flatten() == 0))[1]/np.shape(y_data_strong_connection.flatten())[0],'%')\n",
        "    print(' Dislike edges: ', 100*np.shape(np.where(y_data_strong_connection.flatten() == 1))[1]/np.shape(y_data_strong_connection.flatten())[0],'%')\n",
        "    print(' Indiferent edges: ', 100*np.shape(np.where(y_data_strong_connection.flatten() == 2))[1]/np.shape(y_data_strong_connection.flatten())[0],'%')\n",
        "    print(' Like edges: ', 100*np.shape(np.where(y_data_strong_connection.flatten() == 3))[1]/np.shape(y_data_strong_connection.flatten())[0],'%')\n",
        "    print(' Friends edges: ', 100*np.shape(np.where(y_data_strong_connection.flatten() == 4))[1]/np.shape(y_data_strong_connection.flatten())[0],'%')\n",
        "\n",
        "    num_edges_data = np.shape(y_data_strong_connection)[0]\n",
        "    data_inds = random.sample(range(num_edges_data), num_edges_data)\n",
        "    data_inds_train = data_inds[0:int(proportion_train_test_strong*num_edges_data)]\n",
        "    data_inds_test = data_inds[int(proportion_train_test_strong*num_edges_data):]\n",
        "\n",
        "    x_data_strong_connection_train = x_data_strong_connection[data_inds_train,:]\n",
        "    x_data_strong_connection_test = x_data_strong_connection[data_inds_test,:]\n",
        "\n",
        "    y_data_strong_connection_train = y_data_strong_connection[data_inds_train]\n",
        "    y_data_strong_connection_test = y_data_strong_connection[data_inds_test]\n",
        "\n",
        "    x_data_strong_connection_train = torch.tensor(x_data_strong_connection_train, dtype=torch.int32)\n",
        "    x_data_strong_connection_test = torch.tensor(x_data_strong_connection_test, dtype=torch.int32)\n",
        "\n",
        "    y_data_strong_connection_train = torch.tensor(y_data_strong_connection_train.flatten(), dtype=torch.long)\n",
        "    y_data_strong_connection_test = torch.tensor(y_data_strong_connection_test.flatten(), dtype=torch.long)\n",
        "\n",
        "\n",
        "    if threshold_connection < 1:\n",
        "        return x_data_strong_connection_train,y_data_strong_connection_train,x_data_strong_connection_test,y_data_strong_connection_test\n",
        "    else:\n",
        "        return x_data_strong_connection_train,y_data_strong_connection_train,x_data_strong_connection_test,y_data_strong_connection_test,x_data_weak_connection_train,y_data_weak_connection_train,x_data_weak_connection_test,y_data_weak_connection_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI210jDhW6bh"
      },
      "source": [
        "Training only with two classes of links: label 2 and labels {-1,-2}. For convenience these labels correspond now to 4 and {0,1}. Oversampling the smaller class, same number of samples from each class in each minibatch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxJhPisNW6bi"
      },
      "outputs": [],
      "source": [
        "def separate_two_classes(x_data_train_zeros,y_data_train_zeros,x_data_test_zeros,y_data_test_zeros):\n",
        "      \n",
        "    x_all = x_data_train_zeros.detach().numpy()\n",
        "    print('np.shape(x_all)',np.shape(x_all))\n",
        "\n",
        "    y_all = y_data_train_zeros.detach().numpy()\n",
        "\n",
        "    ind_new_1 = list(np.where(y_all < 2)[0]) \n",
        "    ind_new_2 = list(np.where(y_all == 4)[0])\n",
        "\n",
        "    x_enemies = [x_all[iii] for iii in ind_new_1]\n",
        "    x_friends = [x_all[iii] for iii in ind_new_2]\n",
        "    y_enemies = [0 for iii in ind_new_1] \n",
        "    y_friends = [1 for iii in ind_new_2] \n",
        "\n",
        "    x_two_cl_train = np.array(x_enemies+x_friends)\n",
        "    y_two_cl_train = np.array(y_enemies+y_friends)\n",
        "\n",
        "    ind_enemies = np.array(np.where(y_two_cl_train == 0)[0])\n",
        "    ind_friends = np.array(np.where(y_two_cl_train == 1)[0]) \n",
        "\n",
        "    x_two_cl_train_torch = torch.tensor(x_two_cl_train, dtype=torch.int32)\n",
        "    y_two_cl_train_torch = torch.tensor(y_two_cl_train, dtype=torch.long)\n",
        "\n",
        "\n",
        "    x_all_test = x_data_test_zeros.detach().numpy()\n",
        "    y_all_test = y_data_test_zeros.detach().numpy()\n",
        "\n",
        "    ind_new_1_2 = list(np.where(y_all_test < 2)[0]) \n",
        "    ind_new_2_2 = list(np.where(y_all_test == 4)[0])\n",
        "\n",
        "    x_enemies_2 = [x_all_test[iii] for iii in ind_new_1_2]\n",
        "    x_friends_2 = [x_all_test[iii] for iii in ind_new_2_2]\n",
        "    y_enemies_2 = [0 for iii in ind_new_1_2] \n",
        "    y_friends_2 = [1 for iii in ind_new_2_2] \n",
        "\n",
        "    x_two_cl_test = np.array(x_enemies_2+x_friends_2)\n",
        "    y_two_cl_test = np.array(y_enemies_2+y_friends_2)\n",
        "\n",
        "    x_two_cl_test_torch = torch.tensor(x_two_cl_test, dtype=torch.int32)\n",
        "    y_two_cl_test_torch = torch.tensor(y_two_cl_test, dtype=torch.long)\n",
        "\n",
        "    return x_two_cl_train_torch, y_two_cl_train_torch, x_two_cl_test_torch, y_two_cl_test_torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6DzpoxogkU7"
      },
      "source": [
        "Creating the datasets for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2rWfbrCRjXi"
      },
      "outputs": [],
      "source": [
        "\n",
        "threshold_connection = 1 \n",
        "\n",
        "proportion_train_test_weak = 1 #(not enough data to do validation)\n",
        "proportion_train_test_strong = 0.8\n",
        "\n",
        "x_data_strong_connection_train_col_3,y_data_strong_connection_train_col_3,x_data_strong_connection_test_col_3,y_data_strong_connection_test_col_3,x_data_weak_connection_train_col_3,y_data_weak_connection_train_col_3,x_data_weak_connection_test_col_3,y_data_weak_connection_test_col_3 = separating_data_weak_strong(threshold_connection,proportion_train_test_weak,proportion_train_test_strong,all_edges,mapping_Ids,counting_first_contacts)\n",
        "\n",
        "print('\\n strong connection' )\n",
        "x_two_cl_train_torch_strong_col_3, y_two_cl_train_torch_strong_col_3, x_two_cl_test_torch_strong_col_3, y_two_cl_test_torch_strong_col_3 = separate_two_classes(x_data_strong_connection_train_col_3,y_data_strong_connection_train_col_3,x_data_strong_connection_test_col_3,y_data_strong_connection_test_col_3)\n",
        "\n",
        "print('\\n weak connection' )\n",
        "x_two_cl_train_torch_weak_col_3, y_two_cl_train_torch_weak_col_3, x_two_cl_test_torch_weak_col_3, y_two_cl_test_torch_weak_col_3 = separate_two_classes(x_data_weak_connection_train_col_3,y_data_weak_connection_train_col_3,x_data_weak_connection_test_col_3,y_data_weak_connection_test_col_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcuhpBgGRjXi"
      },
      "outputs": [],
      "source": [
        "friend_influence_torch = torch.tensor(friend_influence, dtype=torch.float)\n",
        "prosocial_sort = prosocial_all_sort\n",
        "CRT_sort = crttotal_all_sort\n",
        "Sexo_sort = Sexo_all_sort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftMNApviDNjf"
      },
      "source": [
        "Training the NN using only the triadic influence between the students. We train 10 NN from scratch, we will average the results below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qcbg2Jp_Swo"
      },
      "outputs": [],
      "source": [
        "GD_folder_results = '.'\n",
        "num_sim_ave = 10\n",
        "\n",
        "final_acc_influ = []\n",
        "models_save_influ = []\n",
        "for ind_ave in range(num_sim_ave):\n",
        "    tr_ac_1_st_influ, tr_ac_w_1_st_influ, test_ac_1_st_influ, test_ac_w_1_st_influ, model_t_99_st_influ = train_NN_new(x_two_cl_train_torch_strong_col_3, y_two_cl_train_torch_strong_col_3, x_two_cl_test_torch_strong_col_3, y_two_cl_test_torch_strong_col_3, train_with = [ ], add_friend_influence = True, proportion_train_test = proportion_train_test_strong)\n",
        "    models_save_influ += [model_t_99_st_influ]\n",
        "    final_acc_influ += [test_ac_w_1_st_influ]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnsoAsVwhC28"
      },
      "source": [
        "Plotting the average probability learnt by the 10 NN trainined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fTQn2dDTrPQ"
      },
      "outputs": [],
      "source": [
        "GD_folder_figures = '.'\n",
        "\n",
        "Z_plot = np.linspace(-50,50,1000)\n",
        "enemies_prob_list = []\n",
        "friends_prob_list = []\n",
        "\n",
        "for mod_i in models_save_influ:\n",
        "    enemies_prob_list += [mod_i.input_skills_only_inf(Z_plot).detach().numpy()[:,0]]\n",
        "    friends_prob_list += [mod_i.input_skills_only_inf(Z_plot).detach().numpy()[:,1]]\n",
        "\n",
        "enemies_prob_list = np.array(enemies_prob_list)\n",
        "friends_prob_list = np.array(friends_prob_list)\n",
        "\n",
        "fig_prob_linear, (a1_pl, a2_pl) = plt.subplots(2, 1, figsize = (20,10), gridspec_kw={'height_ratios': [ 3, 1]})\n",
        "\n",
        "Z_plot = np.linspace(-50,50,1000)\n",
        "\n",
        "enemies_prob_col_1_s = np.mean(enemies_prob_list, axis=0)\n",
        "friends_prob_col_1_s = np.mean(friends_prob_list, axis=0)\n",
        "\n",
        "enemies_prob_std = np.std(enemies_prob_list, axis=0)\n",
        "friends_prob_std = np.std(friends_prob_list, axis=0)\n",
        "\n",
        "color_enemies = (104/255,78/255,170/255)\n",
        "color_friends = (240/255,172/255,79/255)\n",
        "\n",
        "a1_pl.plot(Z_plot,enemies_prob_col_1_s, label = 'Enemies', c = color_enemies)\n",
        "a1_pl.fill_between(Z_plot,enemies_prob_col_1_s-enemies_prob_std,enemies_prob_col_1_s+enemies_prob_std, alpha=0.5, color = color_enemies)\n",
        "\n",
        "a1_pl.plot(Z_plot,friends_prob_col_1_s, label = 'Friends', c = color_friends)\n",
        "a1_pl.fill_between(Z_plot,friends_prob_col_1_s-friends_prob_std,friends_prob_col_1_s+friends_prob_std, alpha=0.5, color = color_friends)\n",
        "\n",
        "a1_pl.set_xticklabels([])\n",
        "\n",
        "a1_pl.set_ylabel('Probability')\n",
        "a1_pl.legend()\n",
        "\n",
        "\n",
        "infl_male_male_friends = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (y_two_cl_train_torch_strong_col_3[aaa] == 1)])\n",
        "infl_male_male_enemies = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (y_two_cl_train_torch_strong_col_3[aaa] == 0)])\n",
        "\n",
        "assess_all = [infl_male_male_enemies,infl_male_male_friends]\n",
        "label_ene_fri = [ 'Enemies', 'Friends']\n",
        "\n",
        "weights_plt=[np.ones(len(assess_all[aaa])) / len(assess_all[aaa]) for aaa in range(len(assess_all))]\n",
        "\n",
        "labels_plot = [label_ene_fri[aaa] for aaa in range(len(assess_all))]\n",
        "\n",
        "a2_pl.hist(assess_all, label=labels_plot, bins=np.linspace(-50,50,51), weights = weights_plt, align='left', color = [color_enemies, color_friends]) #    label=['Male','Female'], bins=np.linspace(0,10,10),bins=range(60), align='left', #,  histtype=u'step', label =  key_ind, color = color_pl)\n",
        "a2_pl.set_ylabel('% relations')\n",
        "a2_pl.set_xlabel('Triadic Influence')\n",
        "a2_pl.legend()\n",
        "\n",
        "fig_prob_linear.tight_layout()\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1oY4UOH7yO8"
      },
      "source": [
        "Plotting the triadic influence distribution depending on the gender of the students forming the relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGM21bsSCwf_"
      },
      "outputs": [],
      "source": [
        "\n",
        "## HISTOGRAMS BY GENDERS\n",
        "\n",
        "# ALL \n",
        "infl_male_male_friends = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (y_two_cl_train_torch_strong_col_3[aaa] == 1)])\n",
        "infl_male_male_enemies = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (y_two_cl_train_torch_strong_col_3[aaa] == 0)])\n",
        "\n",
        "print('len(infl_male_male_friends)',len(infl_male_male_friends))\n",
        "\n",
        "assess_all = [infl_male_male_enemies,infl_male_male_friends]\n",
        "label_ene_fri = [ 'Enemies', 'Friends']\n",
        "\n",
        "fig_hist = plt.figure(figsize=(10,10))\n",
        "\n",
        "weights_plt=[np.ones(len(assess_all[aaa])) / len(assess_all[aaa]) for aaa in range(len(assess_all))]\n",
        "labels_plot = [label_ene_fri[aaa] + ' - mean {:.1f}'.format(np.mean(assess_all[aaa])) for aaa in range(len(assess_all))]\n",
        "\n",
        "plt.hist(assess_all, label=labels_plot, bins=np.linspace(-50,50,51), weights = weights_plt, align='left') #    label=['Male','Female'], bins=np.linspace(0,10,10),bins=range(60), align='left', #,  histtype=u'step', label =  key_ind, color = color_pl)\n",
        "plt.ylabel('% relations')\n",
        "plt.xlabel('Triadic influence')\n",
        "plt.title('All')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "total_friends = 0\n",
        "\n",
        "# MALE-MALE\n",
        "infl_male_male_friends = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 1 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 1 and y_two_cl_train_torch_strong_col_3[aaa] == 1)])\n",
        "infl_male_male_enemies = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 1 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 1 and y_two_cl_train_torch_strong_col_3[aaa] == 0)])\n",
        "\n",
        "total_friends += len(infl_male_male_friends)\n",
        "print('total_friends',total_friends)\n",
        "\n",
        "assess_all = [infl_male_male_enemies,infl_male_male_friends]\n",
        "label_ene_fri = [ 'Enemies', 'Friends']\n",
        "\n",
        "fig_hist = plt.figure(figsize=(10,10))\n",
        "\n",
        "weights_plt=[np.ones(len(assess_all[aaa])) / len(assess_all[aaa]) for aaa in range(len(assess_all))]\n",
        "labels_plot = [label_ene_fri[aaa] + ' - mean {:.1f}'.format(np.mean(assess_all[aaa])) for aaa in range(len(assess_all))]\n",
        "\n",
        "plt.hist(assess_all, label=labels_plot, bins=np.linspace(-50,50,51), weights = weights_plt, align='left') #    label=['Male','Female'], bins=np.linspace(0,10,10),bins=range(60), align='left', #,  histtype=u'step', label =  key_ind, color = color_pl)\n",
        "plt.ylabel('% relations')\n",
        "plt.xlabel('Triadic influence')\n",
        "plt.title('Male-Male')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# FEMALE-FEMALE\n",
        "infl_male_male_friends = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 0 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 0 and y_two_cl_train_torch_strong_col_3[aaa] == 1)])\n",
        "infl_male_male_enemies = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if (Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 0 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 0 and y_two_cl_train_torch_strong_col_3[aaa] == 0)])\n",
        "\n",
        "total_friends += len(infl_male_male_friends)\n",
        "print('total_friends',total_friends)\n",
        "\n",
        "assess_all = [infl_male_male_enemies,infl_male_male_friends]\n",
        "label_ene_fri = [ 'Enemies', 'Friends']\n",
        "\n",
        "fig_hist = plt.figure(figsize=(10,10))\n",
        "\n",
        "weights_plt=[np.ones(len(assess_all[aaa])) / len(assess_all[aaa]) for aaa in range(len(assess_all))]\n",
        "labels_plot = [label_ene_fri[aaa] + ' - mean {:.1f}'.format(np.mean(assess_all[aaa])) for aaa in range(len(assess_all))]\n",
        "\n",
        "plt.hist(assess_all, label=labels_plot, bins=np.linspace(-50,50,51), weights = weights_plt, align='left') #    label=['Male','Female'], bins=np.linspace(0,10,10),bins=range(60), align='left', #,  histtype=u'step', label =  key_ind, color = color_pl)\n",
        "plt.ylabel('% relations')\n",
        "plt.xlabel('Triadic influence')\n",
        "plt.title('Female-Female')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# MALE-FEMALE \n",
        "infl_male_male_friends = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if ((Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 1 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 0) and y_two_cl_train_torch_strong_col_3[aaa] == 1)])\n",
        "infl_male_male_enemies = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if ((Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 1 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 0) and y_two_cl_train_torch_strong_col_3[aaa] == 0)])\n",
        "\n",
        "total_friends += len(infl_male_male_friends)\n",
        "print('total_friends',total_friends)\n",
        "\n",
        "assess_all = [infl_male_male_enemies,infl_male_male_friends]\n",
        "label_ene_fri = [ 'Enemies', 'Friends']\n",
        "\n",
        "fig_hist = plt.figure(figsize=(10,10))\n",
        "\n",
        "weights_plt=[np.ones(len(assess_all[aaa])) / len(assess_all[aaa]) for aaa in range(len(assess_all))]\n",
        "labels_plot = [label_ene_fri[aaa] + ' - mean {:.1f}'.format(np.mean(assess_all[aaa])) for aaa in range(len(assess_all))]\n",
        "\n",
        "plt.hist(assess_all, label=labels_plot, bins=np.linspace(-50,50,51), weights = weights_plt, align='left') #    label=['Male','Female'], bins=np.linspace(0,10,10),bins=range(60), align='left', #,  histtype=u'step', label =  key_ind, color = color_pl)\n",
        "plt.ylabel('% relations')\n",
        "plt.xlabel('Triadic influence')\n",
        "plt.title('Male-Female')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# FEMALE-MALE\n",
        "infl_male_male_friends = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if ((Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 0 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 1) and y_two_cl_train_torch_strong_col_3[aaa] == 1)])\n",
        "infl_male_male_enemies = np.array([ friend_influence_torch[x_two_cl_train_torch_strong_col_3[aaa,0],x_two_cl_train_torch_strong_col_3[aaa,1]]  for aaa in range(x_two_cl_train_torch_strong_col_3.shape[0]) if ((Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,0]] == 0 and Sexo_all_sort[x_two_cl_train_torch_strong_col_3[aaa,1]] == 1) and y_two_cl_train_torch_strong_col_3[aaa] == 0)])\n",
        "\n",
        "total_friends += len(infl_male_male_friends)\n",
        "print('total_friends',total_friends)\n",
        "\n",
        "assess_all = [infl_male_male_enemies,infl_male_male_friends]\n",
        "label_ene_fri = [ 'Enemies', 'Friends']\n",
        "\n",
        "fig_hist = plt.figure(figsize=(10,10))\n",
        "\n",
        "weights_plt=[np.ones(len(assess_all[aaa])) / len(assess_all[aaa]) for aaa in range(len(assess_all))]\n",
        "labels_plot = [label_ene_fri[aaa] + ' - mean {:.1f}'.format(np.mean(assess_all[aaa])) for aaa in range(len(assess_all))]\n",
        "\n",
        "plt.hist(assess_all, label=labels_plot, bins=np.linspace(-50,50,51), weights = weights_plt, align='left') #    label=['Male','Female'], bins=np.linspace(0,10,10),bins=range(60), align='left', #,  histtype=u'step', label =  key_ind, color = color_pl)\n",
        "plt.ylabel('% relations')\n",
        "plt.xlabel('Triadic influence')\n",
        "plt.title('Female-Male')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OimMQdb8Dds4"
      },
      "source": [
        "### Training the NN using only the prosociality of both students"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6S6vlMP7XwF"
      },
      "source": [
        "Training 10 NN from scratch to average later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlxqchxfRjXi"
      },
      "outputs": [],
      "source": [
        "\n",
        "final_acc_pros = []\n",
        "models_save_pros = []\n",
        "for ind_ave in range(num_sim_ave):\n",
        "    tr_ac_1_st_pros, tr_ac_w_1_st_pros, test_ac_1_st_pros, test_ac_w_1_st_pros, model_t_99_st_pros  = train_NN_new(x_two_cl_train_torch_strong_col_3, y_two_cl_train_torch_strong_col_3, x_two_cl_test_torch_strong_col_3, y_two_cl_test_torch_strong_col_3, train_with = [ 'prosocial'], add_friend_influence = False, proportion_train_test = proportion_train_test_strong)\n",
        "    models_save_pros += [model_t_99_st_pros]\n",
        "    final_acc_pros += [test_ac_w_1_st_pros]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5MVyf7d7ei_"
      },
      "source": [
        "Averaging the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7BJXsmlFdY5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "titles = ['Probability Enemies','Probability Friends']\n",
        "\n",
        "def plot_probabilities_paper(net_1,axs_1,fig,train_with):\n",
        "  \n",
        "  number_points_grid = 25\n",
        "  x = np.linspace(0, 1, number_points_grid)\n",
        "  y = np.linspace(0, 1, number_points_grid)\n",
        "  X, Y = np.meshgrid(x, y)\n",
        "\n",
        "  Zs = []\n",
        "  for model_i in net_1:\n",
        "      Z = model_i.input_skills_no_inf(X.flatten(), Y.flatten()).detach().numpy()\n",
        "      Zs += [Z]\n",
        "\n",
        "  Zs = np.array(Zs)\n",
        "\n",
        "  print(np.shape(Z))\n",
        "  print(np.shape(Zs))\n",
        "\n",
        "  Z_mean = np.mean(Zs, axis=0)\n",
        "\n",
        "  Z = Z_mean.reshape(number_points_grid,number_points_grid,Z_mean.shape[1])\n",
        "\n",
        "  for ind_prob in range(0,Z.shape[2]):\n",
        "      max_val = np.max(Z[:,:,ind_prob])\n",
        "      min_val = np.min(Z[:,:,ind_prob])\n",
        "      im = axs_1[ind_prob].contourf(X, Y, Z[:,:,ind_prob], 30, cmap='plasma' )# , vmin=0., vmax=1) #RdGy\n",
        "      axs_1[ind_prob].set_xlabel(train_with[0] + ' (from)')\n",
        "      axs_1[ind_prob].set_ylabel(train_with[0] + ' (to)')\n",
        "      axs_1[ind_prob].set_title(titles[ind_prob])\n",
        "\n",
        "      divider = make_axes_locatable(axs_1[ind_prob])\n",
        "      cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "      cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "\n",
        "      cbar.set_ticks([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8])\n",
        "      plt.tight_layout()\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20,10),\n",
        "                              constrained_layout=True)\n",
        "plot_probabilities_paper(models_save_pros,axs,fig,['prosocial'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MvTLV2HH_-xI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPXIM1gxvkHx4gWSi/6nwE9",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}